import regex as re

class SeparatorTokenizer:
    """
    Простейшая реализация алгоритма токенизации.
    Разделение текста выполняется по заданному разделителю.
    Токены сохраняются в том же порядке, что и исходный текст.
    """

    def __init__(self):
        pass

    def tokenize(self, text: str, separator: str = None) -> list[str]:
        """
        Разбивает строку `text` на токены.

        Пунктуационные символы становятся отдельными токенами. Разделяется строка по `separator`.

        Параметры:
            text      -- исходная строка
            separator -- символ(ы), по которым будет разделён текст

        Возвращает список токенов.
        """
        # Отделяем пунктуацию, чтобы она стала отдельным токеном
        text = re.sub(r'([^\w\s]|_)', r' \1 ', text)
        # Убираем специальные символы перевода строки/табуляции
        text = re.sub(r'[\t\n\r\f\v]', r' ', text)
        return text.split(separator)
