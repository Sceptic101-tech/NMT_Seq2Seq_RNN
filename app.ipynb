{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e998619d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.custom_dataset import CustomDataset\n",
    "from scripts.vectorizer import Seq2Seq_Vectorizer\n",
    "from scripts.tokenizer import SeparatorTokenizer\n",
    "from scripts.vocabulary import Vocabulary\n",
    "from scripts.model import Seq2Seq_Model\n",
    "\n",
    "import numpy as np\n",
    "import gc\n",
    "import os\n",
    "import pandas\n",
    "import time\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "745196c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_PROPORTION = 0.0\n",
    "EVAL_PROPORTION = 0.2\n",
    "\n",
    "TOKENS_TRESHOLD_FREQ = 10\n",
    "\n",
    "SHUFFLE = True\n",
    "DROP_LAST = True\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 0.0001\n",
    "SAMPLE_PROBABILITY = 0.1 # Вероятность при обучении взять в качестве input сгенерированный токен, а не groung-truth токен\n",
    "\n",
    "LR_SCHEDULER_FACTOR = 0.5\n",
    "LR_SCHEDULER_PATIENCE = 2\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "SOURCE_EMBEDDING_SIZE = 48\n",
    "TARGET_EMBEDDING_SIZE = 48\n",
    "MAX_SOURCE_SEQ_LEN = 100\n",
    "MAX_TARGET_SEQ_LEN = 114\n",
    "MAX_GENERATED_SEQ_LEN = 114\n",
    "\n",
    "RNN_HIDDEN_SIZE = 64\n",
    "FC_HIDDEN_SIZE = 256\n",
    "\n",
    "MODEL_SAVE_FILEPATH = 'data/model_params.pt'\n",
    "DATASET_PATH = 'D:/Files/Datasets/NMT_ru_en'\n",
    "\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aac96e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83d8df2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a32492a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(dataset, batch_size, shuffle=True, drop_last=True, device='cpu'):\n",
    "    \"\"\"\n",
    "    Создаёт DataLoader и возвращает батчи в виде словаря.\n",
    "    Для каждой итерации данные сортируются по длине последовательности\n",
    "     и переносятся на нужное устройство.\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(dataset, batch_size, shuffle, drop_last=drop_last)\n",
    "    for data_dict in dataloader:\n",
    "        lengths = data_dict['source_len'].numpy()\n",
    "        sorted_length_indices = lengths.argsort()[::-1].tolist()\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name][sorted_length_indices].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "918b42be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_sizes(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Приводит выход модели и целевые тензоры к совместимым размерам.\n",
    "    Если вывод 3‑D (B, T, vocab), меняем форму на матрицу (BT, vocab).\n",
    "    Цели в виде матрицы (B, T) превращаем в вектор (BT).\n",
    "    \"\"\"\n",
    "    if len(y_pred.size()) == 3:\n",
    "        y_pred = y_pred.reshape(-1, y_pred.size(2))\n",
    "    if len(y_true.size()) == 2:\n",
    "        y_true = y_true.reshape(-1)\n",
    "    return y_pred, y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c59a20be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(y_pred, y_true, mask_index):\n",
    "    \"\"\"\n",
    "    Вычисляет точность предсказаний (в %).\n",
    "    Игнорируются позиции с `mask_index` (т.е. padding‑токены).\n",
    "    \"\"\"\n",
    "    y_pred, y_true = normalize_sizes(y_pred, y_true)\n",
    "\n",
    "    _, y_pred_indices = y_pred.max(dim=1)\n",
    "    \n",
    "    correct_indices = torch.eq(y_pred_indices, y_true).float()\n",
    "    valid_indices = torch.ne(y_true, mask_index).float()\n",
    "    \n",
    "    n_correct = (correct_indices * valid_indices).sum().item()\n",
    "    n_valid = valid_indices.sum().item()\n",
    "\n",
    "    return n_correct / n_valid * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21469582",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_loss(y_pred, y_true, mask_index):\n",
    "    \"\"\"\n",
    "    Кросс‑энтропия для последовательностей.\n",
    "    `ignore_index` позволяет игнорировать padding‑токены в loss.\n",
    "    \"\"\"\n",
    "    y_pred, y_true = normalize_sizes(y_pred, y_true)\n",
    "    return F.cross_entropy(y_pred, y_true, ignore_index=mask_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6a1838d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens_freq(dataframe : pandas.DataFrame) -> tuple[dict, dict]:\n",
    "    \"\"\"\n",
    "    Счётчик частоты токенов по колонкам `source_text` и `target_text`.\n",
    "    Возвращает два словаря: {token: freq}.\n",
    "    \"\"\"\n",
    "    source_freq = {}\n",
    "    target_freq = {}\n",
    "    for i in range(len(dataframe)):\n",
    "        source_tokens, target_tokens = (dataframe.loc[i, 'source_text'], dataframe.loc[i, 'target_text'])\n",
    "        for token in source_tokens:\n",
    "            if token in source_freq:\n",
    "                source_freq[token] += 1\n",
    "            else:\n",
    "                source_freq[token] = 1\n",
    "        for token in target_tokens:\n",
    "            if token in target_freq:\n",
    "                target_freq[token] += 1\n",
    "            else:\n",
    "                target_freq[token] = 1\n",
    "    return source_freq, target_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8912167",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_tokenized_seq_len(dataframe : pandas.DataFrame) -> tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Находит максимальную длину токенизированных строк в колонках\n",
    "    `source_text` и `target_text`. Возвращает два целых числа.\n",
    "    \"\"\"\n",
    "    source_max_len = target_max_len = -1\n",
    "    for idx in range(len(dataframe)):\n",
    "        source_max_len = max(len(dataframe.loc[idx, 'source_text']), source_max_len)\n",
    "        target_max_len = max(len(dataframe.loc[idx, 'target_text']), target_max_len)\n",
    "    return source_max_len, target_max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43da206c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, tokenizer, vectorizer, query: str, max_response_tokens:int=50, response_seq_count : int = 1, temperature: float = 1.0, device='cpu') -> str:\n",
    "    \"\"\"\n",
    "    Генерация ответа модели на входную строку `query`.\n",
    "    Возвращает тензор индексов (batch, seq_len).\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Токенизация и векторизация\n",
    "        tokens = tokenizer.tokenize(query.lower())\n",
    "        vec_dict = vectorizer.vectorize(source_tokens=tokens, use_dataset_max_len=False)\n",
    "\n",
    "        source_vec = torch.tensor(vec_dict['source_vec'], dtype=torch.long).to(device).unsqueeze(0)\n",
    "        # print(f'source_vec.size() {source_vec.size()}')\n",
    "        source_len = torch.tensor([vec_dict['source_len']], dtype=torch.long).to(device)\n",
    "        # print(f'source_len.size() {source_len.size()}')\n",
    "\n",
    "        # Encoder\n",
    "        encoder_state, encoder_final_hidden = model.encoder(source_vec, source_len)\n",
    "\n",
    "        # Decoder\n",
    "        decoded_states = model.decoder(encoder_state, encoder_final_hidden, forced_batch_size=response_seq_count,\\\n",
    "                                    sample_probability=1.0, output_sequence_size=max_response_tokens, temperature=temperature)\n",
    "        \n",
    "        batch_size, seq_size, vocab_size = decoded_states.size()\n",
    "        decoded_states = F.softmax(decoded_states * temperature, dim=-1)\n",
    "        print(f'decoded_states.size() {decoded_states.size()}')\n",
    "        decoded_states = decoded_states.reshape(-1, vocab_size)\n",
    "        indices = torch.multinomial(decoded_states, 1)\n",
    "        print(f'indices.size() {indices.size()}')\n",
    "        indices = indices.reshape(batch_size, seq_size, -1).squeeze(-1)\n",
    "        return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49240e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_indices(indices : torch.tensor, vectorizer):\n",
    "    \"\"\"\n",
    "    Преобразует тензор индексов в читаемый текст.\n",
    "    Останавливает чтение строки при встрече EOS‑токена.\n",
    "    \"\"\"\n",
    "    seq_count, seq_len = (indices.size(0), indices.size(1))\n",
    "    vocab = vectorizer.target_vocab\n",
    "    decoded = []\n",
    "    for seq in range(seq_count):\n",
    "        string =''\n",
    "        for idx in range(seq_len):\n",
    "            index = indices[seq, idx].item()\n",
    "            if index != vocab.mask_token_index:\n",
    "                string += vocab.get_token(index) + ' '\n",
    "            if index == vocab._eos_index:\n",
    "                break\n",
    "        decoded.append(string)\n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3bb1c79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_to_file(model, filepath):\n",
    "    \"\"\"\n",
    "    Сохраняет всю модель (структура + веса) в файл.\n",
    "    \"\"\"\n",
    "    torch.save(model, filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32688bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = pd.read_csv(os.path.join(DATASET_PATH, 'train.csv'))\n",
    "# test_df = pd.read_csv(os.path.join(DATASET_PATH, 'test.csv'))\n",
    "# validation_df = pd.read_csv(os.path.join(DATASET_PATH, 'validation.csv'))\n",
    "# df = pd.concat([train_df, test_df, validation_df], ignore_index=True)\n",
    "# del train_df, test_df, validation_df\n",
    "# gc.collect()\n",
    "# test_valid_len = (TEST_PROPORTION + EVAL_PROPORTION) * len(df)\n",
    "# test_valid_proportion = TEST_PROPORTION / EVAL_PROPORTION\n",
    "# valid_len = test_valid_len / (test_valid_proportion + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6852d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SeparatorTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "094924f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(DATASET_PATH, 'ru_en_small.csv'), index_col='Unnamed: 0')\n",
    "\n",
    "df = df.rename(columns={'ru_text' : 'source_text', 'en_text' : 'target_text'})\n",
    "df['split'] = 'train'\n",
    "selected_indices = df.sample(int(EVAL_PROPORTION*len(df)), random_state=RANDOM_STATE).index\n",
    "df.loc[selected_indices, 'split'] = 'validation'\n",
    "\n",
    "# К нижнему регистру, токенизация и очистка от служебных символов\n",
    "df['source_text'] = df['source_text'].apply(lambda x: tokenizer.tokenize(x.lower()))\n",
    "df['target_text'] = df['target_text'].apply(lambda x: tokenizer.tokenize(x.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "afcf383b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Поиск максимальной длины сурс и таргет текста\n",
    "# source_max_len = target_max_len = -1\n",
    "# for i in range(len(df)):\n",
    "#     source_max_len = max(source_max_len, len(df.loc[i, 'source_text']))\n",
    "#     target_max_len = max(target_max_len, len(df.loc[i, 'target_text']))\n",
    "# print(source_max_len)\n",
    "# print(target_max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bfb99cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source_vocab = Vocabulary()\n",
    "# target_vocab = Vocabulary()\n",
    "\n",
    "# source_freq, target_freq = get_tokens_freq(df)\n",
    "\n",
    "# for key, value in source_freq.items():\n",
    "#     if value > TOKENS_TRESHOLD_FREQ:\n",
    "#         source_vocab.add_token(key)\n",
    "\n",
    "# for key, value in target_freq.items():\n",
    "#     if value > TOKENS_TRESHOLD_FREQ:\n",
    "#         target_vocab.add_token(key)\n",
    "\n",
    "\n",
    "# source_vocab.to_json('data/source_vocab.json')\n",
    "# target_vocab.to_json('data/target_vocab.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c4a81e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_vocab = Vocabulary.from_json('data/source_vocab.json')\n",
    "target_vocab = Vocabulary.from_json('data/target_vocab.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b390ab61",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = Seq2Seq_Vectorizer(source_vocab, target_vocab, MAX_SOURCE_SEQ_LEN, MAX_TARGET_SEQ_LEN)\n",
    "dataset = CustomDataset(df, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c640f4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_vocab_size = len(source_vocab)\n",
    "target_vocab_size = len(target_vocab)\n",
    "mask_index = target_vocab.mask_token_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a0a628c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Seq2Seq_Model(source_vocab_size=source_vocab_size, source_embedding_size=SOURCE_EMBEDDING_SIZE, target_vocab_size=target_vocab_size,\\\n",
    "#                       target_embedding_size=TARGET_EMBEDDING_SIZE, encoder_rnn_size=RNN_HIDDEN_SIZE, fc_hidden_size=FC_HIDDEN_SIZE, target_bos_index=target_vocab._bos_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "16bbd29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(MODEL_SAVE_FILEPATH, weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "62366ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(DEVICE)\n",
    "\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode='min', factor=LR_SCHEDULER_FACTOR, patience=LR_SCHEDULER_PATIENCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ff36bbff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "epoch 0\n",
      "train_epoch_error 495.5587911605835\n",
      "train loss 0.8740013953449445   ,   train accuracy 79.71496327455522\n",
      "validation_epoch_error 147.41282963752747\n",
      "validation loss 1.0454810612590595   ,   validation accuracy 77.77468260208857\n",
      "----------------------------------------\n",
      "epoch 1\n",
      "train_epoch_error 490.12117552757263\n",
      "train loss 0.8644112443167068   ,   train accuracy 79.89346674594118\n",
      "validation_epoch_error 148.01848256587982\n",
      "validation loss 1.0497764720984382   ,   validation accuracy 77.74995858589068\n",
      "----------------------------------------\n",
      "epoch 2\n",
      "train_epoch_error 487.2110313177109\n",
      "train loss 0.859278714846051   ,   train accuracy 80.04840063585054\n",
      "validation_epoch_error 148.83492928743362\n",
      "validation loss 1.0555668743789615   ,   validation accuracy 77.71252186905298\n",
      "----------------------------------------\n",
      "epoch 3\n",
      "train_epoch_error 484.12215983867645\n",
      "train loss 0.8538309697331153   ,   train accuracy 80.11523998376522\n",
      "validation_epoch_error 147.71401071548462\n",
      "validation loss 1.0476170972729408   ,   validation accuracy 77.85172188677238\n",
      "----------------------------------------\n",
      "epoch 4\n",
      "train_epoch_error 481.92079466581345\n",
      "train loss 0.8499484914740977   ,   train accuracy 80.14978413416756\n",
      "validation_epoch_error 151.15480434894562\n",
      "validation loss 1.072019888999614   ,   validation accuracy 77.42215625913833\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    for epoch in range(EPOCHS):\n",
    "        sample_probability = SAMPLE_PROBABILITY\n",
    "        # Iterate over training dataset\n",
    "\n",
    "        # setup: batch generator, set loss and acc to 0, set train mode on\n",
    "        dataset.set_dataframe_split('train')\n",
    "        batch_generator = generate_batches(dataset, batch_size=BATCH_SIZE, shuffle=SHUFFLE, drop_last=DROP_LAST, device=DEVICE)\n",
    "        train_running_loss = 0.0\n",
    "        train_running_acc = 0.0\n",
    "        epoch_err = 0.0\n",
    "        model.train()\n",
    "        \n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # compute the output\n",
    "            y_pred = model(batch_dict['source_vec'],\n",
    "                           batch_dict['source_len'],\n",
    "                           batch_dict['target_x_vec'],\n",
    "                           sample_probability=sample_probability)\n",
    "\n",
    "            # compute the loss\n",
    "            loss = sequence_loss(y_pred, batch_dict['target_y_vec'], mask_index=mask_index)\n",
    "\n",
    "            # use loss to produce gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # use optimizer to take gradient step\n",
    "            optimizer.step()\n",
    "\n",
    "            # compute the running loss and running accuracy\n",
    "            train_running_loss += (loss.item() - train_running_loss) / (batch_index + 1)\n",
    "            epoch_err += loss.item()\n",
    "\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['target_y_vec'], mask_index)\n",
    "            train_running_acc += (acc_t - train_running_acc) / (batch_index + 1)\n",
    "\n",
    "        print('-'*40)\n",
    "        print(f'epoch {epoch}')\n",
    "        print(f'train_epoch_error {epoch_err}')\n",
    "        print(f'train loss {train_running_loss}   ,   train accuracy {train_running_acc}')\n",
    "\n",
    "        # setup: batch generator, set loss and acc to 0; set eval mode on\n",
    "        dataset.set_dataframe_split('validation')\n",
    "        batch_generator = generate_batches(dataset, batch_size=BATCH_SIZE, shuffle=SHUFFLE, drop_last=DROP_LAST, device=DEVICE)\n",
    "        valid_running_loss = 0.0\n",
    "        valid_running_acc = 0.0\n",
    "        epoch_err = 0.0\n",
    "        model.eval()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            # compute the output\n",
    "            y_pred = model(batch_dict['source_vec'],\n",
    "                           batch_dict['source_len'],\n",
    "                           batch_dict['target_x_vec'],\n",
    "                           sample_probability=sample_probability)\n",
    "\n",
    "            # step 3. compute the loss\n",
    "            loss = sequence_loss(y_pred, batch_dict['target_y_vec'], mask_index)\n",
    "\n",
    "            # compute the running loss and accuracy\n",
    "            valid_running_loss += (loss.item() - valid_running_loss) / (batch_index + 1)\n",
    "            epoch_err += loss.item()\n",
    "\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['target_y_vec'], mask_index)\n",
    "            valid_running_acc += (acc_t - valid_running_acc) / (batch_index + 1)\n",
    "\n",
    "        print(f'validation_epoch_error {epoch_err}')\n",
    "        print(f'validation loss {valid_running_loss}   ,   validation accuracy {valid_running_acc}')\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    print(\"Exiting loop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3a1de075",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Я собираюсь поехать в отпуск'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5dccbc27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoded_states.size() torch.Size([1, 114, 4882])\n",
      "indices.size() torch.Size([114, 1])\n"
     ]
    }
   ],
   "source": [
    "indices = generate(model, tokenizer, vectorizer, query, max_response_tokens=MAX_GENERATED_SEQ_LEN, response_seq_count=1, device=DEVICE)\n",
    "response = decode_indices(indices, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3b953a99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"i ' m going to go in vacation . <EOS> \"]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f214eebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_to_file(model, MODEL_SAVE_FILEPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6945d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
